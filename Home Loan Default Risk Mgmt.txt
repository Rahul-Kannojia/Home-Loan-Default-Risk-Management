Numerical Features of Applciation train Dataset:
 ['SK_ID_CURR', 'TARGET', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'HOUR_APPR_PROCESS_START', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']

Categorical Features: ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE'] 

===================================



# creating function to evaluate model
def evaluate_model(true,predicted):
  confusion_matx= confusion_matrix(true,predicted)
  accuracy= accuracy_score(true,predicted)
  f1= f1_score(true,predicted, average='weighted')
  precision= precision_score(true,predicted) 
  recall= recall_score(true,predicted)
  rocauc= roc_auc_score(true,predicted)
  classfication_rep= classification_report(true,predicted)
  return confusion_matx accuracy,f1, precision, recall, rocauc, classfication_rep 
  
  
# Model Training
models={
    'LogisticRegression':LogisticRegression(),
    'RandomForestClassifier':RandomForestClassifier(),
    'XGBClassifier':XGBClassifier(),
	'LightGBM':LGBMClassifier()
}

# Models Training and Evaluation
for i in range(len(list(models))):
    model=list(models.values())[i]
    model.fit(x_train_scaled,y_train)
    # Make Predictions
    y_train_pred=model.predict(x_train_scaled)
    y_test_pred=model.predict(x_test_scaled)
    # Evaluation of Models
    confusion_matx_train, accuracy_train, f1_train, precision_train, recall_train, rocauc_train, classfication_rep_train= evaluate_model(y_train,y_train_pred)
	
    confusion_matx_test, accuracy_test, f1_test, precision_test, recall_test, rocauc_test, classfication_rep_test= evaluate_model(y_test,y_test_pred)
    
    
    print(list(models.keys())[i],':')
    print('Model performance for Training set')
    print("- Confusion Matrix: {:.4f}".format(confusion_matx_train))
    print("- Accuracy: {:.4f}".format(accuracy_train))
    print("- F1-Score: {:.4f}".format(f1_train))
    print("- Precision : {:.4f}".format(precision_train))
	print("- Recall : {:.4f}".format(recall_train))
	print("- ROCAUC : {:.4f}".format(rocauc_train))
	print("- Classification Report : {:.4f}".format(classfication_rep_train))
    print('----------------------------------')
    print('Model performance for Test set')
    print("- Confusion Matrix: {:.4f}".format(confusion_matx_test))
    print("- Accuracy: {:.4f}".format(accuracy_test))
    print("- F1-Score: {:.4f}".format(f1_test))
    print("- Precision : {:.4f}".format(precision_test))
	print("- Recall : {:.4f}".format(recall_test))
	print("- ROCAUC : {:.4f}".format(rocauc_test))
	print("- Classification Report : {:.4f}".format(classfication_rep_test))
    print('='*100)
	
	
	
===========================
# Models Training and Evaluation
for i in range(len(list(models))):
    model = list(models.values())[i]
    model.fit(x_train_scaled, y_train)
    # Make Predictions
    y_train_pred = model.predict(x_train_scaled)
    y_test_pred = model.predict(x_test_scaled)
    # Evaluation of Models
    confusion_matx_train, accuracy_train, f1_train, precision_train, recall_train, rocauc_train, classfication_rep_train = evaluate_model(y_train, y_train_pred)
    
    confusion_matx_test, accuracy_test, f1_test, precision_test, recall_test, rocauc_test, classfication_rep_test = evaluate_model(y_test, y_test_pred)
    
    
    print(list(models.keys())[i], ':')
    print('Model performance for Training set')
    print("- Confusion Matrix: {:.4f}".format(confusion_matx_train))
    print("- Accuracy: {:.4f}".format(accuracy_train))
    print("- F1-Score: {:.4f}".format(f1_train))
    print("- Precision : {:.4f}".format(precision_train))
    print("- Recall : {:.4f}".format(recall_train))
    print("- ROCAUC : {:.4f}".format(rocauc_train))
    print("- Classification Report : {:.4f}".format(classfication_rep_train))
    print('----------------------------------')
    print('Model performance for Test set')
    print("- Confusion Matrix: {:.4f}".format(confusion_matx_test))
    print("- Accuracy: {:.4f}".format(accuracy_test))
    print("- F1-Score: {:.4f}".format(f1_test))
    print("- Precision : {:.4f}".format(precision_test))
    print("- Recall : {:.4f}".format(recall_test))
    print("- ROCAUC : {:.4f}".format(rocauc_test))
    print("- Classification Report : {:.4f}".format(classfication_rep_test))
    print('='*100)


=======================================Hyperparameter Tuning===================


# 1. Initialize with "Regularization" parameters
lgbm_model = LGBMClassifier(
    # --- Control Overfitting ---
    n_estimators=1000,        # High number, but we stop early (see below)
    learning_rate=0.02,       # Slower learning rate = better generalization
    num_leaves=31,            # Keep small (e.g., < 50) to limit complexity
    max_depth=4,              # Hard limit on tree depth
    min_child_samples=200,    # High value prevents learning from "noise" (tiny groups)
    reg_alpha=0.1,            # L1 regularization
    reg_lambda=0.1,           # L2 regularization
    colsample_bytree=0.8,     # Use only 80% of features per tree (adds randomness)
    subsample=0.8,            # Use only 80% of data per tree (adds randomness)
    
    # --- Fix Class Imbalance (Recall = 0 problem) ---
    class_weight='balanced',  # Automatically weights the minority class (Defaults)
    
    # --- Speed ---
    n_jobs=-1,
    random_state=42
)

# 2. Fit with Early Stopping
# This stops training the moment the Validation Score stops improving
lgbm_model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_test, y_test)],
    eval_metric='auc',        # We optimize for AUC, not Accuracy
    callbacks=[
        # Stop if no improvement for 50 rounds
        # Note: In newer LightGBM versions, use the EarlyStopping callback
        # or simply pass early_stopping_rounds in .fit depending on version
    ]
)
# Note: For sklearn API, usually early_stopping_rounds=50 is passed directly in fit()
# lgbm_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=50, eval_metric='auc')

# 3. Predictions
# Use predict_proba for ROC-AUC
y_pred_proba = lgbm_model.predict_proba(X_test)[:, 1]
y_pred_class = lgbm_model.predict(X_test)

# 4. Evaluation
print(f"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")
print(classification_report(y_test, y_pred_class))




